---
title: "¬øQu√© es un Agente?"
---

# ¬øQu√© es un Agente?

## Definici√≥n

Un **agente** es cualquier cosa que puede verse como:
1. **Percibiendo** su entorno a trav√©s de **sensores**
2. **Actuando** sobre ese entorno a trav√©s de **actuadores**

```mermaid
graph LR
    subgraph Environment
        W[World State]
    end
    subgraph Agent
        S[Sensors]
        B[Brain]
        A[Actuators]
    end
    W -->|Percepts| S
    S --> B
    B --> A
    A -->|Actions| W
    
    style W fill:#1e293b,stroke:#94a3b8,color:#e2e8f0
    style S fill:#7dd3fc,stroke:#0284c7,color:#0c4a6e
    style B fill:#a78bfa,stroke:#7c3aed,color:#4c1d95
    style A fill:#f472b6,stroke:#db2777,color:#831843
```

## Ejemplos de Agentes

| Agente | Sensores | Actuadores |
|--------|----------|------------|
| **Humano** | Ojos, o√≠dos, piel | Manos, piernas, voz |
| **Robot** | C√°maras, LIDAR, touch | Motores, grippers |
| **Software** | Archivos, network, input | Display, network, files |
| **Termostato** | Term√≥metro | Switch on/off |

![Ejemplos de Agentes en el Mundo Real]({{ '/02_agentes_&_ambientes/images/ejemplos_agentes.png' | url }})

---

## Clasificaci√≥n de Agentes: Russell & Norvig (AIMA)

La clasificaci√≥n m√°s utilizada en IA proviene de Russell y Norvig en *"Artificial Intelligence: A Modern Approach"*. Clasifica agentes por su **arquitectura interna** ‚Äî c√≥mo procesan informaci√≥n para decidir qu√© hacer.

### Visi√≥n General

```mermaid
graph LR
    A[Simple Reflex] --> B[Model-Based Reflex]
    B --> C[Goal-Based]
    C --> D[Utility-Based]
    D --> E[Learning]
    
    style A fill:#94a3b8,stroke:#475569,color:#1e293b
    style B fill:#7dd3fc,stroke:#0284c7,color:#0c4a6e
    style C fill:#5eead4,stroke:#14b8a6,color:#134e4a
    style D fill:#a78bfa,stroke:#7c3aed,color:#4c1d95
    style E fill:#f472b6,stroke:#db2777,color:#831843
```

Cada tipo **a√±ade capacidades** al anterior:

| Tipo | Usa percept actual | Tiene estado interno | Tiene goals | Tiene utilidad | Aprende |
|------|:------------------:|:--------------------:|:-----------:|:--------------:|:-------:|
| **Simple Reflex** | ‚úì | ‚úó | ‚úó | ‚úó | ‚úó |
| **Model-Based** | ‚úì | ‚úì | ‚úó | ‚úó | ‚úó |
| **Goal-Based** | ‚úì | ‚úì | ‚úì | ‚úó | ‚úó |
| **Utility-Based** | ‚úì | ‚úì | ‚úì | ‚úì | ‚úó |
| **Learning** | ‚úì | ‚úì | ‚úì | ‚úì | ‚úì |

---

### 1. Simple Reflex Agents

El agente m√°s b√°sico. Selecciona acciones bas√°ndose **√∫nicamente en el percept actual**, ignorando toda la historia.

```mermaid
graph LR
    E[Environment] -->|percept| S[Sensors]
    S --> R{Condition-Action Rules}
    R -->|action| A[Actuators]
    A --> E
    
    style E fill:#1e293b,stroke:#94a3b8,color:#e2e8f0
    style S fill:#7dd3fc,stroke:#0284c7,color:#0c4a6e
    style R fill:#94a3b8,stroke:#475569,color:#1e293b
    style A fill:#f472b6,stroke:#db2777,color:#831843
```

**Mecanismo**: Reglas **if-then** (condici√≥n-acci√≥n)

```
function SIMPLE-REFLEX-AGENT(percept):
    rules ‚Üê conjunto de reglas condici√≥n-acci√≥n
    state ‚Üê INTERPRET-INPUT(percept)
    rule ‚Üê RULE-MATCH(state, rules)
    action ‚Üê rule.ACTION
    return action
```

**Ejemplo: Termostato**

| Percept (Temperatura) | Acci√≥n |
|-----------------------|--------|
| T < 18¬∞C | Encender calefacci√≥n |
| T > 22¬∞C | Encender AC |
| 18¬∞C ‚â§ T ‚â§ 22¬∞C | No hacer nada |

**Ejemplo: Vacuum World Simple**

![Vacuum World - El Mundo de la Aspiradora]({{ '/02_agentes_&_ambientes/images/vacuum_world.png' | url }})

| Percept | Acci√≥n |
|---------|--------|
| [A, Dirty] | Suck |
| [A, Clean] | Right |
| [B, Dirty] | Suck |
| [B, Clean] | Left |

**Ventajas**:
- Muy simples de implementar
- Respuesta inmediata (tiempo constante)
- No requieren memoria

**Limitaciones**:
- Solo funcionan en entornos **fully observable**
- No pueden manejar estados ocultos
- Pueden caer en **loops infinitos**

:::example{title="El Problema del Loop"}
En Vacuum World, si el agente est√° en A (limpio) y va a Right, luego en B (limpio) va a Left, ¬°repite infinitamente!

**Soluci√≥n**: Necesita memoria (‚Üí Model-Based)
:::

---

### 2. Model-Based Reflex Agents

Mantiene un **estado interno** que representa aspectos del mundo que no puede ver directamente.

```mermaid
graph TD
    E[Environment] -->|percept| S[Sensors]
    S --> U[Update State]
    M[Internal Model] --> U
    U --> M
    M --> R{Condition-Action Rules}
    R -->|action| A[Actuators]
    A --> E
    
    subgraph "Conocimiento del Agente"
        T1["C√≥mo evoluciona el mundo"]
        T2["C√≥mo mis acciones afectan el mundo"]
    end
    T1 --> U
    T2 --> U
    
    style E fill:#1e293b,stroke:#94a3b8,color:#e2e8f0
    style S fill:#7dd3fc,stroke:#0284c7,color:#0c4a6e
    style U fill:#5eead4,stroke:#14b8a6,color:#134e4a
    style M fill:#7dd3fc,stroke:#0284c7,color:#0c4a6e
    style R fill:#94a3b8,stroke:#475569,color:#1e293b
    style A fill:#f472b6,stroke:#db2777,color:#831843
    style T1 fill:#334155,stroke:#64748b,color:#e2e8f0
    style T2 fill:#334155,stroke:#64748b,color:#e2e8f0
```

**Mecanismo**: Estado interno + reglas

```
function MODEL-BASED-REFLEX-AGENT(percept):
    state ‚Üê UPDATE-STATE(state, action, percept, model)
    rule ‚Üê RULE-MATCH(state, rules)
    action ‚Üê rule.ACTION
    return action
```

El agente necesita dos tipos de conocimiento:
1. **Modelo de transici√≥n**: C√≥mo evoluciona el mundo independientemente del agente
2. **Modelo del sensor**: C√≥mo el estado del mundo se refleja en los percepts

**Ejemplo: Conductor que no ve un carro**

Un conductor en una autopista sabe que hab√≠a un carro en su punto ciego hace 2 segundos. Aunque ahora no lo ve, **mantiene en su estado interno** que probablemente sigue ah√≠.

**Ejemplo: Vacuum World con Memoria**

El agente recuerda qu√© cuartos ya limpi√≥:

| Estado Interno | Percept | Acci√≥n |
|----------------|---------|--------|
| {A: unknown, B: unknown} | [A, Dirty] | Suck, update A:clean |
| {A: clean, B: unknown} | [A, Clean] | Right |
| {A: clean, B: unknown} | [B, Dirty] | Suck, update B:clean |
| {A: clean, B: clean} | [B, Clean] | **Stop** (¬°ya termin√≥!) |

**Ventajas**:
- Funciona en entornos **partially observable**
- Puede evitar loops
- Puede razonar sobre objetos que no ve actualmente

**Limitaciones**:
- El modelo puede ser incorrecto o incompleto
- A√∫n no tiene **objetivos expl√≠citos**
- No puede planificar a futuro

---

### 3. Goal-Based Agents

Adem√°s del estado actual, tiene **objetivos expl√≠citos** que quiere alcanzar. Esto le permite **planificar** y **buscar** secuencias de acciones.

```mermaid
graph TD
    E[Environment] -->|percept| S[Sensors]
    S --> U[Update State]
    M[Internal Model] --> U
    U --> M
    M --> P[Planning/Search]
    G[Goals] --> P
    P -->|action| A[Actuators]
    A --> E
    
    style E fill:#1e293b,stroke:#94a3b8,color:#e2e8f0
    style S fill:#7dd3fc,stroke:#0284c7,color:#0c4a6e
    style U fill:#5eead4,stroke:#14b8a6,color:#134e4a
    style M fill:#7dd3fc,stroke:#0284c7,color:#0c4a6e
    style P fill:#5eead4,stroke:#14b8a6,color:#134e4a
    style G fill:#a78bfa,stroke:#7c3aed,color:#4c1d95
    style A fill:#f472b6,stroke:#db2777,color:#831843
```

**Mecanismo**: B√∫squeda + Planificaci√≥n

```
function GOAL-BASED-AGENT(percept):
    state ‚Üê UPDATE-STATE(state, action, percept, model)
    if goal-achieved(state, goals):
        return NoOp
    plan ‚Üê SEARCH(state, goals, model)
    action ‚Üê first(plan)
    return action
```

**Ejemplo: GPS / Navegaci√≥n**

- **Estado**: Posici√≥n actual, mapa
- **Goal**: Llegar a destino D
- **Proceso**: Buscar ruta √≥ptima de A ‚Üí D
- **Acci√≥n**: Seguir la ruta paso a paso

**Ejemplo: Robot que entrega paquetes**

```
Estado: en(robot, oficina), tiene(robot, paquete)
Goal: en(paquete, almac√©n)

Plan:
1. ir(oficina, pasillo)
2. ir(pasillo, almac√©n)
3. soltar(paquete)
```

**Diferencia clave con Reflex**:

| Situaci√≥n | Reflex Agent | Goal-Based Agent |
|-----------|--------------|------------------|
| Obst√°culo en el camino | Gira a la derecha (regla fija) | Recalcula ruta √≥ptima al goal |
| M√∫ltiples caminos | Siempre elige el mismo | Eval√∫a cu√°l llega al goal |
| Goal cambia | No puede adaptarse | Replanifica autom√°ticamente |

**Ventajas**:
- Comportamiento **flexible** ‚Äî el goal puede cambiar
- Puede manejar **situaciones nuevas**
- Razonamiento sobre **consecuencias** de acciones

**Limitaciones**:
- No distingue entre goals igualmente alcanzables
- ¬øQu√© pasa si hay **m√∫ltiples goals** conflictivos?
- ¬øC√≥mo elegir entre rutas igual de v√°lidas?

---

### 4. Utility-Based Agents

Tiene una **funci√≥n de utilidad** $U: S \to \mathbb{R}$ que mide qu√© tan "feliz" est√° el agente en cada estado. Maximiza la **utilidad esperada**.

```mermaid
graph TD
    E[Environment] -->|percept| S[Sensors]
    S --> U[Update State]
    M[Internal Model] --> U
    U --> M
    M --> EU[Compute Expected Utility]
    UF[Utility Function] --> EU
    EU -->|action that maximizes EU| A[Actuators]
    A --> E
    
    style E fill:#1e293b,stroke:#94a3b8,color:#e2e8f0
    style S fill:#7dd3fc,stroke:#0284c7,color:#0c4a6e
    style U fill:#5eead4,stroke:#14b8a6,color:#134e4a
    style M fill:#7dd3fc,stroke:#0284c7,color:#0c4a6e
    style EU fill:#a78bfa,stroke:#7c3aed,color:#4c1d95
    style UF fill:#fbbf24,stroke:#d97706,color:#78350f
    style A fill:#f472b6,stroke:#db2777,color:#831843
```

**Mecanismo**: Maximizaci√≥n de utilidad esperada

$$a^* = \arg\max_a \sum_{s'} P(s'|s,a) \cdot U(s')$$

**¬øPor qu√© utilidad y no solo goals?**

Los goals son **binarios** (logrado/no logrado). La utilidad permite:
- **Trade-offs** entre objetivos conflictivos
- **Preferencias** sobre c√≥mo lograr el objetivo
- Manejo de **incertidumbre** probabil√≠stica

**Ejemplo: Taxi Aut√≥nomo**

| Factor | Goal-Based | Utility-Based |
|--------|------------|---------------|
| Objetivo | Llegar al destino | $U = f(seguridad, tiempo, costo, comfort)$ |
| Ruta r√°pida pero peligrosa | ¬øV√°lida? | $U = 0.3 \cdot rapido - 0.7 \cdot peligro$ ‚Üí rechazada |
| Ruta lenta pero segura | ¬øV√°lida? | $U = 0.3 \cdot lento + 0.7 \cdot seguro$ ‚Üí aceptada |
| Cliente con prisa | Misma decisi√≥n | Ajustar pesos: $w_{tiempo} \uparrow$ |

**Ejemplo: Decisi√≥n bajo incertidumbre**

Tienes dos rutas:
- **Ruta A**: 30 min seguro
- **Ruta B**: 20 min con 50% de probabilidad, 60 min con 50% (por tr√°fico)

| Tipo de Agente | Decisi√≥n |
|----------------|----------|
| Goal-Based | Ambas llegan ‚Üí ¬øcu√°l elegir? ü§∑ |
| Utility-Based | $E[U_A] = U(30) = 0.7$, $E[U_B] = 0.5 \cdot U(20) + 0.5 \cdot U(60) = 0.55$ ‚Üí Elige A |

**Ventajas**:
- Maneja **m√∫ltiples objetivos** con trade-offs
- Toma decisiones **√≥ptimas bajo incertidumbre**
- Comportamiento **racional** formalmente definido

**Limitaciones**:
- Definir $U$ correctamente es **dif√≠cil**
- Computar la acci√≥n √≥ptima puede ser **costoso**
- El agente no mejora con la experiencia

---

### 5. Learning Agents

Puede **mejorar su comportamiento** a trav√©s de la experiencia. Tiene componentes adicionales para aprender.

```mermaid
graph TD
    E[Environment] -->|percept| S[Sensors]
    S --> PE[Performance Element]
    PE -->|action| A[Actuators]
    A --> E
    
    E -->|feedback| C[Critic]
    C -->|learning goals| LE[Learning Element]
    LE -->|changes| PE
    LE -->|experiments| PG[Problem Generator]
    PG -->|exploratory actions| A
    
    style E fill:#1e293b,stroke:#94a3b8,color:#e2e8f0
    style S fill:#7dd3fc,stroke:#0284c7,color:#0c4a6e
    style PE fill:#5eead4,stroke:#14b8a6,color:#134e4a
    style A fill:#f472b6,stroke:#db2777,color:#831843
    style C fill:#fbbf24,stroke:#d97706,color:#78350f
    style LE fill:#a78bfa,stroke:#7c3aed,color:#4c1d95
    style PG fill:#fb7185,stroke:#e11d48,color:#881337
```

**Componentes**:

| Componente | Funci√≥n | Ejemplo |
|------------|---------|---------|
| **Performance Element** | Selecciona acciones (como los agentes anteriores) | El "cerebro" actual |
| **Critic** | Eval√∫a qu√© tan bien lo est√° haciendo | Compara con est√°ndar de performance |
| **Learning Element** | Modifica el Performance Element | Actualiza reglas, modelo, utilidad |
| **Problem Generator** | Sugiere acciones exploratorias | "¬øQu√© pasa si pruebo esto?" |

**Tipos de Aprendizaje**:

| Qu√© aprende | Ejemplo |
|-------------|---------|
| **Reglas condici√≥n-acci√≥n** | Aprender que "cielo oscuro ‚Üí llevar paraguas" |
| **Modelo del mundo** | Aprender c√≥mo el tr√°fico afecta tiempos de viaje |
| **Funci√≥n de utilidad** | Aprender preferencias del usuario |
| **Goals** | Descubrir qu√© objetivos son importantes |

**Ejemplo: Sistema de Recomendaci√≥n**

```
D√≠a 1: Recomienda pel√≠culas aleatorias
       Usuario da ratings
       Critic: "Le gustaron las de acci√≥n, no le gustaron las rom√°nticas"
       Learning Element: Actualiza modelo de preferencias
       
D√≠a 30: Recomienda pel√≠culas de acci√≥n con alta precisi√≥n
        Problem Generator: "¬øY si pruebo una comedia de acci√≥n?"
        ‚Üí Descubre nuevo gusto del usuario
```

**Ejemplo: AlphaGo**

1. **Performance Element**: Red neuronal que eval√∫a posiciones + MCTS
2. **Critic**: ¬øGan√≥ o perdi√≥ la partida?
3. **Learning Element**: Backpropagation, actualiza pesos de la red
4. **Problem Generator**: Self-play genera nuevas situaciones

**Ventajas**:
- Puede operar en **entornos desconocidos**
- **Mejora** con el tiempo
- Puede descubrir **estrategias no anticipadas** por el dise√±ador

**Limitaciones**:
- Necesita **mucha experiencia** para aprender bien
- Puede aprender **comportamientos no deseados**
- El dise√±o del sistema de aprendizaje es complejo

---

### Resumen: Progresi√≥n de Arquitecturas

```mermaid
graph TB
    subgraph "Capacidad Creciente"
        SR[Simple Reflex] -->|+estado interno| MB[Model-Based]
        MB -->|+goals| GB[Goal-Based]
        GB -->|+utilidad| UB[Utility-Based]
        UB -->|+aprendizaje| LA[Learning Agent]
    end
    
    SR ---|Ejemplo| T1[Termostato]
    MB ---|Ejemplo| T2[Conductor humano]
    GB ---|Ejemplo| T3[GPS navegador]
    UB ---|Ejemplo| T4[Taxi aut√≥nomo]
    LA ---|Ejemplo| T5[AlphaGo]
    
    style SR fill:#94a3b8,stroke:#475569,color:#1e293b
    style MB fill:#7dd3fc,stroke:#0284c7,color:#0c4a6e
    style GB fill:#5eead4,stroke:#14b8a6,color:#134e4a
    style UB fill:#a78bfa,stroke:#7c3aed,color:#4c1d95
    style LA fill:#f472b6,stroke:#db2777,color:#831843
    style T1 fill:#334155,stroke:#64748b,color:#e2e8f0
    style T2 fill:#334155,stroke:#64748b,color:#e2e8f0
    style T3 fill:#334155,stroke:#64748b,color:#e2e8f0
    style T4 fill:#334155,stroke:#64748b,color:#e2e8f0
    style T5 fill:#334155,stroke:#64748b,color:#e2e8f0
```

| Pregunta | Simple Reflex | Model-Based | Goal-Based | Utility-Based | Learning |
|----------|---------------|-------------|------------|---------------|----------|
| ¬øQu√© hay ahora? | ‚úì | ‚úì | ‚úì | ‚úì | ‚úì |
| ¬øQu√© pasar√° si hago X? | ‚úó | ‚úì | ‚úì | ‚úì | ‚úì |
| ¬øQu√© quiero lograr? | ‚úó | ‚úó | ‚úì | ‚úì | ‚úì |
| ¬øQu√© tan bueno es? | ‚úó | ‚úó | ‚úó | ‚úì | ‚úì |
| ¬øC√≥mo puedo mejorar? | ‚úó | ‚úó | ‚úó | ‚úó | ‚úì |

---

## Clasificaci√≥n Alternativa: Weiss (1999)

Gerhard Weiss en *"Multiagent Systems"* ofrece una perspectiva complementaria, clasificando agentes por **propiedades observables** y **arquitectura cognitiva**.

### Propiedades Fundamentales de un Agente

Para Weiss, un agente "verdadero" debe tener estas propiedades:

| Propiedad | Descripci√≥n | Ejemplo |
|-----------|-------------|---------|
| **Autonom√≠a** | Opera sin intervenci√≥n directa | Un robot que decide cu√°ndo recargar bater√≠a |
| **Reactividad** | Responde a cambios del entorno oportunamente | Un carro que frena ante un obst√°culo |
| **Pro-actividad** | Toma iniciativa, no solo reacciona | Un asistente que sugiere tareas sin que le preguntes |
| **Habilidad Social** | Interact√∫a con otros agentes | Un bot de trading que negocia con otros bots |

### Tipos de Agentes seg√∫n Weiss

```mermaid
graph LR
    subgraph Reactivos["üî¥ Agentes Reactivos"]
        R1[Sin modelo del mundo]
        R2[Respuesta est√≠mulo-acci√≥n]
        R3[R√°pidos pero limitados]
    end
    subgraph Deliberativos["üîµ Agentes Deliberativos"]
        D1[Modelo simb√≥lico del mundo]
        D2[Razonamiento y planificaci√≥n]
        D3[Arquitectura BDI]
    end
    subgraph Hibridos["üü¢ Agentes H√≠bridos"]
        H1[Capas reactivas]
        H2[Capas deliberativas]
        H3[Lo mejor de ambos]
    end
    
    style R1 fill:#fb7185,stroke:#e11d48,color:#881337
    style R2 fill:#fb7185,stroke:#e11d48,color:#881337
    style R3 fill:#fb7185,stroke:#e11d48,color:#881337
    style D1 fill:#7dd3fc,stroke:#0284c7,color:#0c4a6e
    style D2 fill:#7dd3fc,stroke:#0284c7,color:#0c4a6e
    style D3 fill:#7dd3fc,stroke:#0284c7,color:#0c4a6e
    style H1 fill:#5eead4,stroke:#14b8a6,color:#134e4a
    style H2 fill:#5eead4,stroke:#14b8a6,color:#134e4a
    style H3 fill:#5eead4,stroke:#14b8a6,color:#134e4a
```

**1. Agentes Reactivos**
- No tienen modelo interno del mundo
- Mapeo directo percepci√≥n ‚Üí acci√≥n
- Inspirados en comportamiento de insectos
- Ejemplo: Arquitectura de subsunci√≥n de Brooks

**2. Agentes Deliberativos**
- Mantienen un modelo simb√≥lico del mundo
- Razonan sobre ese modelo para decidir
- Arquitectura **BDI** (Beliefs-Desires-Intentions):
  - **Beliefs**: Lo que el agente cree sobre el mundo
  - **Desires**: Los estados que quiere alcanzar
  - **Intentions**: Los planes que se ha comprometido a ejecutar

**3. Agentes H√≠bridos**
- Combinan capas reactivas (respuesta r√°pida) con deliberativas (planificaci√≥n)
- La capa reactiva maneja emergencias
- La capa deliberativa planifica a largo plazo
- Ejemplo: Arquitectura InteRRaP, TouringMachines

---

## Comparaci√≥n: Russell & Norvig vs Weiss

| Aspecto | Weiss | Russell & Norvig |
|---------|-------|------------------|
| **Pregunta central** | "¬øQu√© propiedades tiene?" | "¬øC√≥mo decide qu√© hacer?" |
| **Enfoque** | Comportamiento observable | Mecanismo interno |
| **Contexto** | Sistemas multiagente | Agente individual |
| **Categor√≠as** | 3 tipos (reactivo, deliberativo, h√≠brido) | 5 tipos progresivos |

#### Mapeo entre Clasificaciones

```mermaid
graph TD
    subgraph Weiss["Weiss"]
        W1[Reactivo]
        W2[Deliberativo]
        W3[H√≠brido]
    end
    subgraph RN["Russell & Norvig"]
        RN1[Simple Reflex]
        RN2[Model-Based]
        RN3[Goal-Based]
        RN4[Utility-Based]
        RN5[Learning]
    end
    W1 -.->|corresponde a| RN1
    W1 -.->|corresponde a| RN2
    W2 -.->|corresponde a| RN3
    W2 -.->|corresponde a| RN4
    W3 -.->|combina| RN2
    W3 -.->|combina| RN3
    RN5 -.->|puede agregarse a| W1
    RN5 -.->|puede agregarse a| W2
    RN5 -.->|puede agregarse a| W3
    
    style W1 fill:#fb7185,stroke:#e11d48,color:#881337
    style W2 fill:#7dd3fc,stroke:#0284c7,color:#0c4a6e
    style W3 fill:#5eead4,stroke:#14b8a6,color:#134e4a
    style RN1 fill:#94a3b8,stroke:#475569,color:#1e293b
    style RN2 fill:#7dd3fc,stroke:#0284c7,color:#0c4a6e
    style RN3 fill:#5eead4,stroke:#14b8a6,color:#134e4a
    style RN4 fill:#a78bfa,stroke:#7c3aed,color:#4c1d95
    style RN5 fill:#f472b6,stroke:#db2777,color:#831843
```

#### Diferencias Clave

| Criterio | Weiss | Russell & Norvig |
|----------|-------|------------------|
| **Modelo interno** | Reactivo=no, Deliberativo=s√≠ | Reflex=no, Model-based+=s√≠ |
| **Planificaci√≥n** | Solo deliberativos | Goal-based y Utility-based |
| **Aprendizaje** | No es categor√≠a separada | Es un tipo adicional |
| **Interacci√≥n social** | Propiedad fundamental | No es central |
| **BDI** | Arquitectura espec√≠fica para deliberativos | No se menciona expl√≠citamente |

---

### ¬øCu√°l usamos en este curso?

> **Seguimos la clasificaci√≥n de Russell & Norvig** porque:
> 1. Progresi√≥n clara de capacidades
> 2. Conecta directamente con las t√©cnicas del curso
> 3. Es el est√°ndar en cursos introductorios

Pero la perspectiva de **Weiss aporta**:
- El concepto de **autonom√≠a** como propiedad fundamental
- La arquitectura **BDI** para agentes deliberativos
- La importancia de la **habilidad social** en sistemas reales
- El dise√±o de **agentes h√≠bridos** con capas

:::exercise{title="Clasifica seg√∫n Weiss y R&N" difficulty="2"}

Para cada agente, clasif√≠calo seg√∫n **ambas** taxonom√≠as:

1. **Roomba b√°sica** (aspiradora robot simple)
   - Weiss: ¬øReactivo, Deliberativo, o H√≠brido?
   - R&N: ¬øSimple Reflex, Model-Based, Goal-Based, Utility-Based?

2. **GPS de carro** (calcula rutas)
   - Weiss: ?
   - R&N: ?

3. **Carro aut√≥nomo Waymo**
   - Weiss: ?
   - R&N: ?

4. **AlphaGo** (juega Go)
   - Weiss: ?
   - R&N: ?

5. **ChatGPT**
   - Weiss: ?
   - R&N: ?

Justifica cada clasificaci√≥n.

:::

<details>
<summary><strong>Ver Respuestas</strong></summary>

#### 1. Roomba b√°sica

| Taxonom√≠a | Clasificaci√≥n | Justificaci√≥n |
|-----------|---------------|---------------|
| **Weiss** | **Reactivo** | Responde directamente a sensores: ¬øsucio? aspirar. ¬øobst√°culo? girar. No planea ni mantiene modelo complejo. |
| **R&N** | **Simple Reflex** (b√°sica) o **Model-Based** (avanzada) | Las Roombas b√°sicas usan reglas condici√≥n-acci√≥n. Las nuevas construyen mapas ‚Üí Model-Based. |

#### 2. GPS de carro

| Taxonom√≠a | Clasificaci√≥n | Justificaci√≥n |
|-----------|---------------|---------------|
| **Weiss** | **Deliberativo** | Razona sobre un modelo del mundo (mapa), planifica rutas, no solo reacciona. |
| **R&N** | **Goal-Based** (o **Utility-Based**) | Tiene objetivo expl√≠cito (destino), usa b√∫squeda/planificaci√≥n. Si optimiza tiempo/distancia/tr√°fico ‚Üí Utility-Based. |

#### 3. Carro aut√≥nomo Waymo

| Taxonom√≠a | Clasificaci√≥n | Justificaci√≥n |
|-----------|---------------|---------------|
| **Weiss** | **H√≠brido** | Combina capas reactivas (frenado de emergencia) con deliberativas (planificaci√≥n de ruta, predicci√≥n de otros veh√≠culos). |
| **R&N** | **Utility-Based + Learning** | Maximiza utilidad (seguridad √ó velocidad √ó comodidad √ó legalidad), aprende de datos, mantiene modelo del mundo. |

#### 4. AlphaGo

| Taxonom√≠a | Clasificaci√≥n | Justificaci√≥n |
|-----------|---------------|---------------|
| **Weiss** | **H√≠brido** | Tiene componentes reactivos (red de policy aprendida) y deliberativos (MCTS para planificaci√≥n). |
| **R&N** | **Learning + Utility-Based** | Aprendi√≥ de self-play, usa funci√≥n de valor (utilidad), planifica con MCTS. |

#### 5. ChatGPT

| Taxonom√≠a | Clasificaci√≥n | Justificaci√≥n |
|-----------|---------------|---------------|
| **Weiss** | **Reactivo** (principalmente) | No mantiene memoria entre sesiones, responde al input actual. Aunque tiene patrones complejos aprendidos. |
| **R&N** | **Learning** (entrenamiento) ‚Üí desplegado como **Model-Based Reflex** sofisticado | Fue entrenado (learning), pero en uso no tiene goals expl√≠citos ni optimiza utilidad en el sentido cl√°sico. Es un caso l√≠mite interesante. |

#### Nota sobre ChatGPT

Los LLMs no encajan perfectamente en las taxonom√≠as cl√°sicas de agentes:
- No tienen **estado persistente** (no son model-based en el sentido tradicional)
- No tienen **objetivos expl√≠citos** (no son goal-based)
- No optimizan una **utilidad definida** en cada interacci√≥n
- Pero s√≠ **aprendieron** patrones complejos del mundo

Esto es un tema de discusi√≥n activo en AI: ¬øSon los LLMs agentes? ¬øQu√© tipo? La respuesta depende de c√≥mo los integremos en sistemas m√°s amplios.

</details>

---

## Agent Function vs Agent Program

Dos conceptos relacionados pero distintos:

| Concepto | Descripci√≥n | Naturaleza |
|----------|-------------|------------|
| **Agent Function** | Mapeo de percept sequences a actions | Abstracta, matem√°tica |
| **Agent Program** | Implementaci√≥n concreta | C√≥digo ejecutable |

$$f: \mathcal{P}^* \rightarrow \mathcal{A}$$

Donde $\mathcal{P}^*$ es el conjunto de todas las secuencias posibles de percepts y $\mathcal{A}$ es el conjunto de acciones.

### El Problema de la Tabla

Si intent√°ramos implementar un agente con una **tabla de lookup**:

```
Percept Sequence ‚Üí Action
[A, Clean] ‚Üí Right
[A, Dirty] ‚Üí Suck
[B, Clean] ‚Üí Left
[B, Dirty] ‚Üí Suck
[A, Clean], [A, Clean] ‚Üí Right
...
```

Para un taxi aut√≥nomo con c√°mara HD a 30fps por 1 hora:
- Entradas posibles: $> 10^{600,000,000,000}$
- √Åtomos en el universo observable: $< 10^{80}$

#### ¬øDe d√≥nde sale ese n√∫mero astron√≥mico?

Calculemos paso a paso:

1. **Un frame de video HD**:
   - Resoluci√≥n: $1920 \times 1080 = 2,073,600$ p√≠xeles
   - Cada p√≠xel tiene 3 canales (RGB)
   - Cada canal tiene 256 valores posibles (8 bits)
   - **Bits por frame**: $1920 \times 1080 \times 3 \times 8 \approx 50 \times 10^6$ bits

2. **Una hora de video a 30fps**:
   - Frames totales: $30 \times 60 \times 60 = 108,000$ frames
   - **Bits totales**: $50 \times 10^6 \times 108,000 \approx 5.4 \times 10^{12}$ bits

3. **N√∫mero de posibles secuencias de percepts**:
   - Cada secuencia de $n$ bits puede tomar $2^n$ valores
   - Posibles historias: $2^{5.4 \times 10^{12}}$

4. **Convertir a base 10**:
   - $2^n = 10^{n \cdot \log_{10}(2)} = 10^{n \cdot 0.301}$
   - $2^{5.4 \times 10^{12}} = 10^{5.4 \times 10^{12} \times 0.301} \approx 10^{1.6 \times 10^{12}}$

Eso es $10^{1,600,000,000,000}$ ‚Äî ¬°diez elevado a 1.6 **trillones**!

El n√∫mero $10^{600,000,000,000}$ en el texto es una estimaci√≥n conservadora (asumiendo menos bits por frame o compresi√≥n). El punto es el mismo: **es incomprensiblemente m√°s grande que cualquier cosa f√≠sica**.

> **Perspectiva**: Si cada √°tomo en el universo fuera una computadora, y cada computadora pudiera almacenar un estado por cada nanosegundo desde el Big Bang... no alcanzar√≠as ni a rozar $10^{600,000,000,000}$.

**Conclusi√≥n**: Necesitamos programas compactos, no tablas.

---

## Visualizaci√≥n: Espectro de Agentes

![Espectro de Agentes]({{ '/02_agentes_&_ambientes/images/espectro_de_agentes.png' | url }})

---

:::exercise{title="Identificando Agentes" difficulty="1"}

Para cada sistema, identifica:
1. ¬øEs un agente? ¬øPor qu√©?
2. Si s√≠, ¬øcu√°les son sus sensores y actuadores?

Sistemas:
- a) Un reloj de pared
- b) Un termostato programable
- c) Una puerta autom√°tica
- d) Google Search
- e) Un virus biol√≥gico
- f) Una empresa (como organizaci√≥n)

:::

---

:::exercise{title="Vacuum World" difficulty="2"}

Considera el mundo de la aspiradora con dos cuartos (A y B):

```
‚îå‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îê
‚îÇ A ‚îÇ B ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îò
```

Cada cuarto puede estar limpio o sucio. La aspiradora puede:
- Moverse a la izquierda (Left)
- Moverse a la derecha (Right)
- Aspirar (Suck)

1. ¬øCu√°ntos estados posibles tiene el environment?
2. ¬øCu√°ntas percept sequences de longitud 3 existen?
3. Dise√±a una agent function simple (tabla peque√±a) que limpie ambos cuartos.

:::

---

:::prompt{title="Explorar Agentes con LLM" for="Claude/ChatGPT"}

Quiero entender mejor el concepto de agente en IA. 

Dado el siguiente sistema: [DESCRIBE TU SISTEMA]

1. ¬øPuede modelarse como un agente? Justifica.
2. Si s√≠, ¬øcu√°les ser√≠an sus:
   - Sensores (¬øqu√© percibe?)
   - Actuadores (¬øqu√© acciones puede tomar?)
   - Environment (¬øen qu√© mundo opera?)
3. ¬øQu√© tan complejo es su "agent function"?
4. ¬øQu√© desaf√≠os tendr√≠a implementar este agente?

Dame ejemplos concretos y espec√≠ficos.

:::

---

## Puntos Clave

1. **Agente = Percepci√≥n + Acci√≥n** en un environment
2. La **agent function** es el mapeo ideal; el **agent program** es la implementaci√≥n
3. Las tablas de lookup son **imposibles** para problemas reales
4. El reto de AI es encontrar **programas compactos** que aproximen buenas agent functions

